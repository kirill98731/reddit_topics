{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97e875e2",
   "metadata": {},
   "source": [
    "# Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ea624fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import praw\n",
    "import json\n",
    "import requests\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db59973",
   "metadata": {},
   "source": [
    "Text clearing function\n",
    "\n",
    "* Lowercase the text\n",
    "* Remove unicode characters\n",
    "* Remove stop words\n",
    "* Remove mentions\n",
    "* Remove URL\n",
    "* Remove Hashtags\n",
    "* Remove ticks and the next character\n",
    "* Remove punctuations\n",
    "* Remove numbers\n",
    "* Replace the over spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "f4dba6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_text(x):\n",
    "    x = x.lower()\n",
    "    x = ' '.join([word for word in x.split(' ') if word not in stop_words])\n",
    "    x = x.encode('ascii', 'ignore').decode()\n",
    "    x = re.sub(r'https*\\S+', ' ', x)\n",
    "    x = re.sub(r'@\\S+', ' ', x)\n",
    "    x = re.sub(r'#\\S+', ' ', x)\n",
    "    x = re.sub(r'\\'\\w+', '', x)\n",
    "    x = re.sub('[%s]' % re.escape(string.punctuation), ' ', x)\n",
    "    x = re.sub(r'\\w*\\d+\\w*', '', x)\n",
    "    x = re.sub(r'\\s{2,}', ' ', x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7902cf",
   "metadata": {},
   "source": [
    "Data request function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8aba3719",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_request(uri, max_retries = 5):\n",
    "    def fire_away(uri):\n",
    "        response = requests.get(uri)\n",
    "        assert response.status_code == 200\n",
    "        return json.loads(response.content)\n",
    "    current_tries = 1\n",
    "    while current_tries < max_retries:\n",
    "        try:\n",
    "            time.sleep(1)\n",
    "            response = fire_away(uri)\n",
    "            return response\n",
    "        except:\n",
    "            time.sleep(1)\n",
    "            current_tries += 1\n",
    "    return fire_away(uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed569946",
   "metadata": {},
   "source": [
    "Collecting the required data\n",
    "* **title**: The title of the submission\n",
    "* **score**: The number of upvotes for the submission\n",
    "* **id**: ID of the submission\n",
    "* **url**: The URL the submission links to\n",
    "* **num_comments**: The number of comments on the submission\n",
    "* **created_utc**: Time the submission was created\n",
    "* **selftext**: The submissions’ selftext        \n",
    "* **author**: Provides an instance of Redditor\n",
    "* **is_self**: Whether or not the submission is a selfpost (text-only)\n",
    "* **subreddit**: Provides an instance of Subreddit\n",
    "* **cleared_text**: Cleared selftext,\n",
    "* **link_flair_text**: The link flair’s text content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4705b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_posts(posts):\n",
    "    return list(map(lambda post: {\n",
    "        'title': post['title'],\n",
    "        'score': post['score'],\n",
    "        'id': post['id'],\n",
    "        'url': post['url'],\n",
    "        'num_comments': post['num_comments'],\n",
    "        'created_utc': post['created_utc'],\n",
    "        'selftext': post.get('selftext', ''),        \n",
    "        'author': post['author'],\n",
    "        'is_self': post['is_self'],\n",
    "        'subreddit': post['subreddit'],\n",
    "        'cleared_text': '',\n",
    "        'link_flair_text': post.get('link_flair_text', '')\n",
    "    }, posts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789e1c9d",
   "metadata": {},
   "source": [
    "Function for selecting documents with a number of characters greater than the threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6d2b703",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_post(post_collections):\n",
    "    post_collections_cleared = []\n",
    "    for post in post_collections:\n",
    "        if post['is_self'] and len(post['selftext'])>=2000: \n",
    "            cleared_text = clear_text(post['selftext'])\n",
    "            if len(cleared_text)>=2000:\n",
    "                post['cleared_text'] = cleared_text\n",
    "                post_collections_cleared.append(post)\n",
    "    return post_collections_cleared"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca66098c",
   "metadata": {},
   "source": [
    "A function that returns all the documents of a certain community for a certain period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bdae3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_posts_for(subreddit, start_at, end_at):\n",
    "    SIZE = 100\n",
    "    URI_TEMPLATE = r'https://api.pushshift.io/reddit/search/submission?subreddit={}&after={}&before={}&size={}'\n",
    "    it = 0\n",
    "    post_collections = map_posts(make_request(URI_TEMPLATE.format(subreddit, start_at, end_at, SIZE))['data'])\n",
    "    n_1 = len(post_collections)\n",
    "    last = post_collections[-1]\n",
    "    post_collections = get_post(post_collections)\n",
    "    n_2 = len(post_collections)\n",
    "    \n",
    "    while n_1 == SIZE and n_2 <= 10000:\n",
    "        new_start_at = last['created_utc'] + (5)\n",
    "        more_posts = map_posts(make_request(URI_TEMPLATE.format(subreddit, new_start_at, end_at, SIZE))['data'])\n",
    "        n_1 = len(more_posts)\n",
    "        last = more_posts[-1]\n",
    "        more_posts = get_post(more_posts)        \n",
    "        post_collections.extend(more_posts)\n",
    "        n_2 = len(post_collections)\n",
    "        it+=1\n",
    "    print(it, n_1, n_2)\n",
    "    return post_collections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0e7679",
   "metadata": {},
   "source": [
    "Selected communities, because they are dominated by documents with a large number of characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41f2f35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_subreddit = [\n",
    "    'relationships',\n",
    "    'love',\n",
    "    'family',\n",
    "    'Marriage',\n",
    "    'Parenting',\n",
    "    'askwomenadvice',\n",
    "    'DecidingToBeBetter',\n",
    "    'depression',\n",
    "    'SuicideWatch',\n",
    "    'TwoXChromosomes'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e17a76c",
   "metadata": {},
   "source": [
    "# Data collection\n",
    "The request returns no more than 100 documents, we make requests until we get the required number of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7be209b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relationships 19200 10026 10026\n",
      "love 32500 20035 10009\n",
      "family 17500 30037 10002\n",
      "Marriage 25400 40039 10002\n",
      "Parenting 21900 50043 10004\n",
      "askwomenadvice 27600 60080 10037\n",
      "DecidingToBeBetter 26100 70108 10028\n",
      "depression 29600 80143 10035\n",
      "SuicideWatch 31300 90159 10016\n",
      "TwoXChromosomes 31100 100180 10021\n",
      "Wall time: 2h 25min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "post_collections = []\n",
    "end_at = math.ceil(datetime.utcnow().timestamp())\n",
    "start_at = math.floor((datetime.utcnow() - timedelta(days=730)).timestamp())\n",
    "URI_TEMPLATE = r'https://api.pushshift.io/reddit/search/submission?subreddit={}&after={}&before={}&size={}'\n",
    "SIZE = 100\n",
    "for subreddit in set_subreddit:\n",
    "    it = 0\n",
    "    n_3 = 0\n",
    "    more_posts = map_posts(make_request(URI_TEMPLATE.format(subreddit, start_at, end_at, SIZE))['data'])\n",
    "    n_1 = len(more_posts)\n",
    "    last = more_posts[-1]\n",
    "    n_temp = len(post_collections)\n",
    "    post_collections.extend(get_post(more_posts))\n",
    "    n_2 = len(post_collections)\n",
    "    n_3+=(n_2-n_temp)\n",
    "    while n_1 == SIZE and n_3<=10000:\n",
    "        new_start_at = last['created_utc'] + (5)\n",
    "        more_posts = map_posts(make_request(URI_TEMPLATE.format(subreddit, new_start_at, end_at, SIZE))['data'])\n",
    "        n_1 = len(more_posts)\n",
    "        last = more_posts[-1]\n",
    "        n_temp = len(post_collections)\n",
    "        post_collections.extend(get_post(more_posts))\n",
    "        n_2 = len(post_collections)\n",
    "        n_3+=(n_2-n_temp)\n",
    "        it+=1\n",
    "    print(subreddit, it*SIZE+n_1, n_2, n_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "f2a40ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_data = pd.DataFrame(post_collections)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad18461c",
   "metadata": {},
   "source": [
    "# Export data to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "63cb6f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_data['title_cleared'] = posts_data['title'].apply(clear_text)\n",
    "posts_data.to_csv(\"posts.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
